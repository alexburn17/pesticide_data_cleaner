import platform
import platform
# make sure python 3.9 is loaded
print(platform.python_version())
from spacetime.input.readData import read_data
from spacetime.scale.rasterAlign import raster_align
from spacetime.scale.rasterTrim import raster_trim
from spacetime.objects.fileObject import file_object
from spacetime.operations.cubeSmasher import cube_smasher
from spacetime.operations.cubeSmasher import cube_smasher
from spacetime.operations.makeCube import make_cube
from spacetime.operations.loadCube import load_cube
from spacetime.graphics.dataPlot import plot_cube
from spacetime.graphics.dataPlot import plot_cube
from spacetime.operations.time import cube_time, return_time, scale_time, select_time
from spacetime.operations.cubeToDataframe import cube_to_dataframe
library(tidyverse)
library(stringr)
library(raster)
months <- c(paste0("0",seq(1,9)),10,11,12)
years <- seq(1981,1995)
precip <- c(paste0("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/monthly/pr/CHELSA_pr_",
rep(months,30),"_",rep(years,each = 12),"_V.2.1.tif"))
tmin <- c(paste0("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/monthly/tasmin/CHELSA_tasmin_",
rep(months,15),"_",rep(years,each = 12),"_V.2.1.tif"))
tmax <- c(paste0("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/monthly/tasmax/CHELSA_tasmax_",
rep(months,15),"_",rep(years,each = 12),"_V.2.1.tif"))
txt_file <- c(precip, tmin, tmax)
txt_file
# raster I use to define cropping extent
ref_extent <- raster("D:/Climate/CHELSA2/bio1_1981-2010_V.2.1.tif")
# raster I use to define cropping extent
ref_extent <- raster("/Users/pburnham/Documents/geospatialData/Carya_ovata/Carya_ovata_sim_disc_10km.tif")
# raster I use to define cropping extent
ref_extent <- raster("/Users/pburnham/Documents/geospatialData/Carya_ovata/Carya_ovata_sim_disc_10km.tif")
plot(ref_extent)
for(f in c(1:length(txt_file))){
# Download file from CHELSA
url <- txt_file[f]
name <- gsub(".*/CHELSA_","",url)
dest_file <- paste("D:/Climate/CHELSA_BIOCLIM+/",name,sep ="")
download.file(url=url, destfile=dest_file, method="auto", quiet = F, mode = "wb", cacheOK = TRUE)
# Crop and save cropped file
org_file <- raster(dest_file)
e <- extent(ref_extent)
cropped_file <- crop(org_file,e)
writeRaster(cropped_file, filename=dest_file, overwrite=TRUE)
print(name)
}
z
for(f in c(1:length(txt_file))){
# Download file from CHELSA
url <- txt_file[f]
name <- gsub(".*/CHELSA_","",url)
print(name)
dest_file <- paste("D:/Climate/CHELSA_BIOCLIM+/",name,sep ="")
download.file(url=url, destfile=dest_file, method="auto", quiet = F, mode = "wb", cacheOK = TRUE)
# Crop and save cropped file
org_file <- raster(dest_file)
e <- extent(ref_extent)
cropped_file <- crop(org_file,e)
writeRaster(cropped_file, filename=dest_file, overwrite=TRUE)
print(name)
}
for(f in c(1:length(txt_file))){
# Download file from CHELSA
url <- txt_file[f]
name <- gsub(".*/CHELSA_","",url)
dest_file <- paste("/Users/pburnham/Documents/geospatialData/chelsa_data/",name,sep ="")
download.file(url=url, destfile=dest_file, method="auto", quiet = F, mode = "wb", cacheOK = TRUE)
# Crop and save cropped file
org_file <- raster(dest_file)
e <- extent(ref_extent)
cropped_file <- crop(org_file,e)
writeRaster(cropped_file, filename=dest_file, overwrite=TRUE)
}
for(f in c(1:length(txt_file))){
print(f)
# Download file from CHELSA
url <- txt_file[f]
name <- gsub(".*/CHELSA_","",url)
dest_file <- paste("/Users/pburnham/Documents/geospatialData/chelsa_data/",name,sep ="")
download.file(url=url, destfile=dest_file, method="auto", quiet = F, mode = "wb", cacheOK = TRUE)
# Crop and save cropped file
org_file <- raster(dest_file)
e <- extent(ref_extent)
cropped_file <- crop(org_file,e)
writeRaster(cropped_file, filename=dest_file, overwrite=TRUE)
print(name)
}
getOption('timeout')
options(timeout=100)
getOption('timeout')
for(f in c(1:length(txt_file))){
print(f)
# Download file from CHELSA
url <- txt_file[f]
name <- gsub(".*/CHELSA_","",url)
dest_file <- paste("/Users/pburnham/Documents/geospatialData/chelsa_data/",name,sep ="")
download.file(url=url, destfile=dest_file, method="auto", quiet = F, mode = "wb", cacheOK = TRUE)
# Crop and save cropped file
org_file <- raster(dest_file)
e <- extent(ref_extent)
cropped_file <- crop(org_file,e)
writeRaster(cropped_file, filename=dest_file, overwrite=TRUE)
print(name)
}
options(timeout=100)
txt_file
install.packages("roxygen2")
setwd("/Users/pburnham/Desktop")
ds <- read.csv("pest_DescResults_combined.csv", header = TRUE, stringsAsFactors = FALSE)
ds
ds$value
ds[ds$value >=0, ]
ds[!is.na(ds$value), ]
postive <- ds[!is.na(ds$value), ]
postive
View(postive)
setwd("/Users/pburnham/Documents/GitHub/pesticide_data_cleaner")
library(tidyverse)
library(dplyr)
library(ggplot2)
library(reshape2)
setwd("~/Documents/GitHub/pesticide_data_cleaner")
# Read in Cornell Results Dataset !NOTE! - find more general solution to white space/column headers
pest_Results <- read.csv("data/pesticide_results_2021.csv", header = TRUE,
stringsAsFactors = FALSE, skip = 1)
# Read in Tosi Datasets
tosi_lethal <- read.csv("data/Tosi_lethal.csv", header = TRUE, stringsAsFactors = FALSE, skip = 1)
tosi_sublethal <- read.csv("data/Tosi_sublethal.csv", header = TRUE, stringsAsFactors = FALSE)
# Read in Description Dataset (NHBS descriptions)
pest_Desc <- read.csv("data/pestDesc.csv", header = TRUE, stringsAsFactors = FALSE)
# Read in additional description information (classification info in Google Sheet from Colin)
pest_Desc_additionalinfo <- read.csv("data/pestDesc_additioninfo.csv", header = TRUE, stringsAsFactors = FALSE)
# Read in updated description information - Colin
pest_Desc_updated <- read.csv("data/updated_descriptions_4-1-23.csv")
# replace n.d. with NA
pest_Results[pest_Results == "n.d."] <- NA
# find our cut points
LS_row <- which(pest_Results == "Large Scale", arr.ind=TRUE)[1]
SS_row <- which(pest_Results == "Small Scale", arr.ind=TRUE)[1]
SS_filename_row <- which(pest_Results == "File Name", arr.ind=TRUE)[1]
bottom_row <- which(pest_Results == "Results are in ppb.", arr.ind=TRUE)[1] #find regex solution Results are in *
# cut out our data frames
LS_df <- pest_Results[1:(LS_row-1),]
SS_df <- pest_Results[(SS_filename_row+1):(SS_row-1),]
# cut out look up tables
LS_lookup <- pest_Results[LS_row:(LS_row+3),]
SS_lookup <- pest_Results[SS_row:(SS_row+3),]
#######################################################
# LIMIT FINDER FUNCTION
#######################################################
limit_finder <- function(df, search, lookup, scale){
# find where samples say <loq
loqVals <- data.frame(which(df == search, arr.ind=TRUE))
if(length(loqVals$row>0)){
# pull out mass and lod and do out the division
mass <- as.numeric(df$Mass..g.[loqVals$row])
scaleNum <- ifelse(search==">ULOQ", 3, 1) # convert scale into row index
print(scaleNum)
lod <- as.numeric(lookup[scaleNum,loqVals$col])
results <- lod/mass
# assign results to index where loq was found
for(i in 1:length(results)){
df[loqVals$row[i], loqVals$col[i]] <- results[i]
}
}
return(df)
}
LS_df <- limit_finder(df = LS_df, search = "<LOQ", lookup = LS_lookup)
number_adder(num1 = 2, num2 = 3)
source("R/addNumbers.R")
number_adder(num1 = 2, num2 = 3)
